import numpy as np
import tensorflow as tf
from numpy import array
from pickle import dump
from tensorflow.keras.utils import to_categorical

#first step is to load the data

def load_data(data):
    file = open(data, "r")
    text = file.read()
    file.close()
    return text
def save_data(data, filename):
    file = open(filename, "w")
    file.write(data)
    file.close

text = load_data("Text Files and Dictionary/Sixpence.txt")
textLines = text.split()
joinedText = " ".join(textLines)

#Create the sequence of data that is going to be feed into the RNN. These would later be
#seperated into input and output vectors
#What are the three dimensional inputs like?

seqLen = 15 #toggle between different values
sequence = list()
for i in range(seqLen, len(joinedText)):
    seq = joinedText[i - seqLen : i+1]
    sequence.append(seq)

print("Length of sequence is: %d " % len(sequence))

data_to_save = "\n".join(sequence)
save_data(data_to_save, "Text Files and Dictionary/Sixpence sequence.txt")

#encode the sequences into integers
#First we need to determine the vocabulary size for the tetx. How many unique characters are there

#load the sequence file
sequenceData = load_data("Text Files and Dictionary/Sixpence sequence.txt")
sequenceList = sequenceData.split("\n")
print(sequenceData)
#filter all the unique characters by putting it into a set

charSet = sorted(set(sequenceData))
dictionary = dict((char, index) for index, char in enumerate(charSet))
vocabSize = len(dictionary)

#We can integer encode each line in the sequence now as:
integer_sequence = list()
for line in sequenceList:
    seq = [dictionary[char] for char in line]
    integer_sequence.append(seq)
#print(integer_sequence)

#next we need to one hot encode each sequence such that it is a 0-1 vector with length of the vocabulary size
#with a one representing the point where the character exists.
integer_sequence = array(integer_sequence)
x, y = integer_sequence[:, :-1], integer_sequence[:, -1]
#print(x)
int_sequences = [to_categorical(num, num_classes=vocabSize) for num in x]
x = array(int_sequences)
y= to_categorical(y, num_classes=vocabSize)


#Here we prepare the test data, I'm sepaaring it because

arrayShape = open("Text Files and Dictionary/arrayShape.txt", "w")
arrayShape.write(" ".join(str(x) for x in x.shape))
arrayShape.write("\n")
arrayShape.write(" ".join(str(y) for y in y.shape))

np.array(x).tofile("Text Files and Dictionary/x.bin")
np.array(y).tofile("Text Files and Dictionary/y.bin")

dump(dictionary, open("Text Files and Dictionary/dictionarySP.pkl", "wb"))



//GRU Model

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.python.keras.layers import GRU
from tensorflow.python.keras.layers import Dense
from pickle import load


arrayShape = open("../Text Files and Dictionary/arrayShape.txt", "r")
shape = arrayShape.read().split("\n")

g = shape[0].split(" ")
for t in shape[0].split(" "):
    print(int(t))
xShape = tuple(list(map(int, shape[0].split(" "))))
#print(xShape)
yShape = tuple(list(map(int, shape[1].split(" "))))
#print(yShape)


x = np.fromfile("../Text Files and Dictionary/x.bin", dtype=np.float32)
x = x.reshape(xShape)


y = np.fromfile("../Text Files and Dictionary/y.bin", dtype=np.float32)
y = y.reshape(yShape)

dictionary = load(open("../Text Files and Dictionary/dictionarySP.pkl", "rb"))

model = Sequential()
model.add(GRU (76, input_shape=(x.shape[1], x.shape[2])))
model.add(Dense (len(dictionary), activation="softmax"))
print(model.summary())
#How do you choose the amount of memory cells for the GRU?
#How do you know what activation is best

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
model.fit(x, y, epochs= 120, verbose=2)

model.save("../Learned Models/learnedModelGRU.h5")



//GRU Model 2

import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.python.keras.layers import GRU
from tensorflow.python.keras.layers import Dense
from pickle import load


arrayShape = open("../Text Files and Dictionary/arrayShape.txt", "r")
shape = arrayShape.read().split("\n")


xTrainShape = tuple(list(map(int, shape[0].split(" "))))
#print(xShape)
yTrainShape = tuple(list(map(int, shape[1].split(" "))))
#print(yShape)
xTestShape = tuple(list(map(int, shape[2].split(" "))))
#print(xShape)
yTestShape = tuple(list(map(int, shape[3].split(" "))))
#print(yShape)


x_train = np.fromfile("../Text Files and Dictionary/xTrain.bin", dtype=np.float32)
x_train = x_train.reshape(xTrainShape)

y_train= np.fromfile("../Text Files and Dictionary/yTrain.bin", dtype=np.float32)
y_train = y_train.reshape(yTrainShape)

x_test= np.fromfile("../Text Files and Dictionary/xTest.bin", dtype=np.float32)
x_test = x_test.reshape(xTestShape)


y_test= np.fromfile("../Text Files and Dictionary/yTest.bin", dtype=np.float32)
y_test = y_test.reshape(yTestShape)

dictionary = load(open("../Text Files and Dictionary/dictionarySP.pkl", "rb"))

model = Sequential()
model.add(GRU (90, input_shape=(x_train.shape[1], x_train.shape[2])))
model.add(Dense (len(dictionary), activation="softmax"))
print(model.summary())
#How do you choose the amount of memory cells for the LSTM?
#How do you know what activation is best

model.compile(loss="categorical_crossentropy", optimizer="adam", metrics=["accuracy"])
history = model.fit(x_train, y_train, epochs= 100, verbose=2, validation_data=(x_test, y_test))
results = model.evaluate(x_test, y_test)
print("test loss, test acc:", results)
print(history.history)
model.save("../Learned Models/learnedModelGRU.h5")


IN Get gates.py

# for index,char in enumerate(text_to_predict):
#
#     if(index < 2):
#         sequence += char
#         predictedTextGRU, h_tm1_gru= generateSeqGRU(modelGRUFunc, 5, 2, sequence)
#         print(predictedTextGRU)
#
#         predictedTextLSTM, h_tm1_lstm, c_tm1_lstm = generateSeqLSTM(modelLSTMFunc, 5, 2, sequence)
#         print(predictedTextLSTM)
#         continue
#
#     print("   GRU prediction")
#     h_tm1_gru = get_gatesGRU(weightGRUFunc, encode_sequence(1, char), h_tm1_gru)
#     predictedNo = resultModelGRU.predict(array(h_tm1_gru).reshape(1, 1, hidden_units))
#     # if(index ==3): print (predictedNo)
#    # print(predictedNo)
#     print(predictedChar(predictedNo))
#
#     print("   LSTM prediction")
#     h_tm1_lstm , c_tm1_lstm, f= get_gatesLSTM(weightLSTMFunc, encode_sequence(1, char), h_tm1_lstm, c_tm1_lstm)
#     predictedNo = resultModelLSTM.predict(array(h_tm1_lstm).reshape(1, 1, hidden_units))
#     print(predictedChar(predictedNo))
#
#     f_t.append(f)
